{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnXi+dDdq0T+YqMro43VnQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitaDeshmukh2024/SPPU_DSBDA_Practicals/blob/main/7/DSBDA_Assign7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK**\n",
        "\n",
        "NLTK stands for Natural Language Toolkit (NLTK) library.\n",
        "\n",
        "**punkt**: This provides the Punkt tokenizer models for tokenizing text into sentences. Tokenization is the process of splitting a text into individual words or tokens.\n",
        "\n",
        "**averaged_perceptron_tagger**: This provides the averaged perceptron tagger model for part-of-speech (POS) tagging. POS tagging involves labeling each word in a sentence with its corresponding part of speech (e.g., noun, verb, adjective).\n",
        "\n",
        "**stopwords**: This provides a list of common stopwords for various languages. Stopwords are words that are considered to be uninformative or irrelevant in text analysis tasks and are often removed from text before further processing.\n",
        "\n",
        "**wordnet**: This provides the WordNet lexical database, which is a large lexical database of English words and their semantic relationships. WordNet is useful for tasks such as synonym detection, word sense disambiguation, and semantic similarity calculation.\n",
        "\n",
        "import FreqDist class from probability module from nltk.\n",
        "\n",
        "The **FreqDist** class is used to represent frequency distributions of items in a sequence, such as words in a text. The FreqDist class is useful for analyzing the distribution of words (or other items) in a text and identifying frequently occurring items."
      ],
      "metadata": {
        "id": "qSiLgDHy09S2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wVT-JNPqWf5",
        "outputId": "1444d5f1-8ae4-49fb-a733-b9e3853d9bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = \" Pollution is a term which even kids are aware of these days. It has become so common that almost everyone acknowledges the fact that pollution is rising continuously. The term 'pollution' means the manifestation of any unsolicited foreign substance in something. When we talk about pollution on earth, we refer to the contamination that is happening of the natural resources by various pollutants. All this is mainly caused by human activities which harm the environment in ways more than one.  \""
      ],
      "metadata": {
        "id": "15Mf_i1zsMVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word_tokenize**\n",
        "\n",
        "imports the word_tokenize class from the tokenize module of nltk.\n",
        "\n",
        "**Word tokenization** is the process of breaking down a text or a document into individual words or tokens. These tokens are the basic units of text processing and analysis, and they serve as the building blocks for various natural language processing tasks.\n",
        "\n",
        "Word tokenization typically involves identifying word boundaries in the text, including splitting words based on spaces, punctuation, and other delimiters. However, it also considers special cases such as contractions, hyphenated words, and abbreviations."
      ],
      "metadata": {
        "id": "ohfa-dQN2c-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(document)\n",
        "for token in tokens :\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8XcohQOt16R",
        "outputId": "a5ae88e4-31ca-446f-bc8c-b0161044837c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pollution\n",
            "is\n",
            "a\n",
            "term\n",
            "which\n",
            "even\n",
            "kids\n",
            "are\n",
            "aware\n",
            "of\n",
            "these\n",
            "days\n",
            ".\n",
            "It\n",
            "has\n",
            "become\n",
            "so\n",
            "common\n",
            "that\n",
            "almost\n",
            "everyone\n",
            "acknowledges\n",
            "the\n",
            "fact\n",
            "that\n",
            "pollution\n",
            "is\n",
            "rising\n",
            "continuously\n",
            ".\n",
            "The\n",
            "term\n",
            "'pollution\n",
            "'\n",
            "means\n",
            "the\n",
            "manifestation\n",
            "of\n",
            "any\n",
            "unsolicited\n",
            "foreign\n",
            "substance\n",
            "in\n",
            "something\n",
            ".\n",
            "When\n",
            "we\n",
            "talk\n",
            "about\n",
            "pollution\n",
            "on\n",
            "earth\n",
            ",\n",
            "we\n",
            "refer\n",
            "to\n",
            "the\n",
            "contamination\n",
            "that\n",
            "is\n",
            "happening\n",
            "of\n",
            "the\n",
            "natural\n",
            "resources\n",
            "by\n",
            "various\n",
            "pollutants\n",
            ".\n",
            "All\n",
            "this\n",
            "is\n",
            "mainly\n",
            "caused\n",
            "by\n",
            "human\n",
            "activities\n",
            "which\n",
            "harm\n",
            "the\n",
            "environment\n",
            "in\n",
            "ways\n",
            "more\n",
            "than\n",
            "one\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pos_tag**\n",
        "\n",
        "imports the pos_tag module of nltk.\n",
        "\n",
        "This function is part of NLTK's pos_tag module. It assigns a part-of-speech tag (such as noun, verb, adjective, etc.) to each token in the input text.\n",
        "\n",
        "Each element in the pos_tags list will be a tuple containing a token and its corresponding part-of-speech tag.\n",
        "\n",
        "DT = determiner\n",
        "\n",
        "NN = Noun, singular or mass\n",
        "\n",
        "VBZ = Verb, 3rd person singular present\n",
        "\n",
        "WDT = Wh-determiner\n",
        "\n",
        "RR = Adverb\n",
        "\n",
        "NNS = Noun, plural\n",
        "\n",
        "VBP = Verb, non-3rd person singular present\n",
        "\n",
        "JJ = Adjective\n",
        "\n",
        "IN = Preposition or subordinating conjunction\n",
        "\n",
        "PRP = Personal pronoun"
      ],
      "metadata": {
        "id": "XeFpGL2j3tkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(tokens)\n",
        "for tag in pos_tags :\n",
        "  print(tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDiod-SnuB5e",
        "outputId": "2494873e-f5b8-4e56-a257-4f2a2d49109e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Pollution', 'NN')\n",
            "('is', 'VBZ')\n",
            "('a', 'DT')\n",
            "('term', 'NN')\n",
            "('which', 'WDT')\n",
            "('even', 'RB')\n",
            "('kids', 'NNS')\n",
            "('are', 'VBP')\n",
            "('aware', 'JJ')\n",
            "('of', 'IN')\n",
            "('these', 'DT')\n",
            "('days', 'NNS')\n",
            "('.', '.')\n",
            "('It', 'PRP')\n",
            "('has', 'VBZ')\n",
            "('become', 'VBN')\n",
            "('so', 'RB')\n",
            "('common', 'JJ')\n",
            "('that', 'IN')\n",
            "('almost', 'RB')\n",
            "('everyone', 'NN')\n",
            "('acknowledges', 'VBZ')\n",
            "('the', 'DT')\n",
            "('fact', 'NN')\n",
            "('that', 'IN')\n",
            "('pollution', 'NN')\n",
            "('is', 'VBZ')\n",
            "('rising', 'VBG')\n",
            "('continuously', 'RB')\n",
            "('.', '.')\n",
            "('The', 'DT')\n",
            "('term', 'NN')\n",
            "(\"'pollution\", 'NNP')\n",
            "(\"'\", 'POS')\n",
            "('means', 'VBZ')\n",
            "('the', 'DT')\n",
            "('manifestation', 'NN')\n",
            "('of', 'IN')\n",
            "('any', 'DT')\n",
            "('unsolicited', 'JJ')\n",
            "('foreign', 'JJ')\n",
            "('substance', 'NN')\n",
            "('in', 'IN')\n",
            "('something', 'NN')\n",
            "('.', '.')\n",
            "('When', 'WRB')\n",
            "('we', 'PRP')\n",
            "('talk', 'VBP')\n",
            "('about', 'IN')\n",
            "('pollution', 'NN')\n",
            "('on', 'IN')\n",
            "('earth', 'NN')\n",
            "(',', ',')\n",
            "('we', 'PRP')\n",
            "('refer', 'VBP')\n",
            "('to', 'TO')\n",
            "('the', 'DT')\n",
            "('contamination', 'NN')\n",
            "('that', 'WDT')\n",
            "('is', 'VBZ')\n",
            "('happening', 'VBG')\n",
            "('of', 'IN')\n",
            "('the', 'DT')\n",
            "('natural', 'JJ')\n",
            "('resources', 'NNS')\n",
            "('by', 'IN')\n",
            "('various', 'JJ')\n",
            "('pollutants', 'NNS')\n",
            "('.', '.')\n",
            "('All', 'PDT')\n",
            "('this', 'DT')\n",
            "('is', 'VBZ')\n",
            "('mainly', 'RB')\n",
            "('caused', 'VBN')\n",
            "('by', 'IN')\n",
            "('human', 'JJ')\n",
            "('activities', 'NNS')\n",
            "('which', 'WDT')\n",
            "('harm', 'VBP')\n",
            "('the', 'DT')\n",
            "('environment', 'NN')\n",
            "('in', 'IN')\n",
            "('ways', 'NNS')\n",
            "('more', 'JJR')\n",
            "('than', 'IN')\n",
            "('one', 'CD')\n",
            "('.', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords**\n",
        "\n",
        "imports the stopwords class from the corpus module of nltk.\n",
        "\n",
        "**nltk.corpus** is a module within the Natural Language Toolkit (NLTK) library that provides access to a collection of linguistic resources, such as text corpora, lexical resources, and treebanks. These resources are commonly used in natural language processing (NLP) tasks for research, education, and development purposes.\n",
        "\n",
        "The nltk.corpus module offers a wide range of datasets and resources that cover various languages, genres, and domains.\n",
        "\n",
        "This code will print each stopword from the **English stopwords** corpus on a separate line. Stopwords are common words like \"the,\" \"is,\" \"and,\" \"of,\" etc., which are often filtered out from text data during preprocessing because they typically don't contribute much to the meaning of the text and can be considered noise in certain natural language processing tasks."
      ],
      "metadata": {
        "id": "qu5nGqLp7NiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "for word in stop_words :\n",
        "  print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPyKGIrCvdD6",
        "outputId": "158a82b5-27ed-4622-e916-2889ec0f7f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "haven\n",
            "you'd\n",
            "couldn\n",
            "should've\n",
            "them\n",
            "that'll\n",
            "here\n",
            "no\n",
            "so\n",
            "then\n",
            "haven't\n",
            "how\n",
            "over\n",
            "yourself\n",
            "hasn't\n",
            "do\n",
            "be\n",
            "our\n",
            "nor\n",
            "needn't\n",
            "t\n",
            "she's\n",
            "aren\n",
            "shouldn't\n",
            "above\n",
            "wasn't\n",
            "herself\n",
            "m\n",
            "didn't\n",
            "most\n",
            "by\n",
            "at\n",
            "mustn't\n",
            "these\n",
            "wasn\n",
            "not\n",
            "out\n",
            "there\n",
            "themselves\n",
            "she\n",
            "can\n",
            "when\n",
            "aren't\n",
            "ours\n",
            "once\n",
            "few\n",
            "yourselves\n",
            "y\n",
            "on\n",
            "if\n",
            "himself\n",
            "myself\n",
            "shan't\n",
            "before\n",
            "through\n",
            "are\n",
            "other\n",
            "s\n",
            "he\n",
            "my\n",
            "has\n",
            "an\n",
            "that\n",
            "mightn't\n",
            "been\n",
            "wouldn't\n",
            "again\n",
            "the\n",
            "mightn\n",
            "hadn't\n",
            "is\n",
            "until\n",
            "as\n",
            "and\n",
            "will\n",
            "ve\n",
            "of\n",
            "theirs\n",
            "couldn't\n",
            "very\n",
            "should\n",
            "same\n",
            "ourselves\n",
            "off\n",
            "yours\n",
            "itself\n",
            "down\n",
            "you\n",
            "all\n",
            "doesn't\n",
            "further\n",
            "we\n",
            "have\n",
            "too\n",
            "with\n",
            "because\n",
            "from\n",
            "am\n",
            "shouldn\n",
            "isn\n",
            "him\n",
            "such\n",
            "a\n",
            "hers\n",
            "needn\n",
            "in\n",
            "during\n",
            "its\n",
            "it's\n",
            "both\n",
            "hasn\n",
            "now\n",
            "ain\n",
            "wouldn\n",
            "me\n",
            "were\n",
            "those\n",
            "to\n",
            "being\n",
            "who\n",
            "only\n",
            "between\n",
            "own\n",
            "re\n",
            "whom\n",
            "his\n",
            "this\n",
            "it\n",
            "below\n",
            "or\n",
            "don\n",
            "don't\n",
            "they\n",
            "did\n",
            "more\n",
            "than\n",
            "ll\n",
            "didn\n",
            "your\n",
            "had\n",
            "weren\n",
            "hadn\n",
            "doing\n",
            "under\n",
            "any\n",
            "into\n",
            "about\n",
            "where\n",
            "you'll\n",
            "won\n",
            "you're\n",
            "o\n",
            "her\n",
            "against\n",
            "which\n",
            "d\n",
            "while\n",
            "weren't\n",
            "why\n",
            "but\n",
            "i\n",
            "their\n",
            "after\n",
            "some\n",
            "what\n",
            "mustn\n",
            "does\n",
            "ma\n",
            "you've\n",
            "was\n",
            "up\n",
            "each\n",
            "having\n",
            "just\n",
            "for\n",
            "shan\n",
            "won't\n",
            "isn't\n",
            "doesn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line creates a new list called filtered_tokens using list comprehension. It iterates over each word in the tokens list and checks if the lowercase version of the word is not in the set of stopwords (stop_words). If the word is not a stopword, it is included in the filtered_tokens list.\n",
        "\n",
        "The purpose of this code is to remove stopwords from the list of tokens (tokens) and print the remaining tokens. Stopwords are common words like \"the,\" \"is,\" \"and,\" \"of,\" etc., which are often filtered out from text data during preprocessing because they typically don't contribute much to the meaning of the text. Filtering stopwords can help focus on the more meaningful words in the text for further analysis or processing."
      ],
      "metadata": {
        "id": "eXGg9r_F9gOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "for token in filtered_tokens :\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG9FRQSmv-CD",
        "outputId": "2e32e88f-0ac2-428f-df5d-310c3f4e99fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pollution\n",
            "term\n",
            "even\n",
            "kids\n",
            "aware\n",
            "days\n",
            ".\n",
            "become\n",
            "common\n",
            "almost\n",
            "everyone\n",
            "acknowledges\n",
            "fact\n",
            "pollution\n",
            "rising\n",
            "continuously\n",
            ".\n",
            "term\n",
            "'pollution\n",
            "'\n",
            "means\n",
            "manifestation\n",
            "unsolicited\n",
            "foreign\n",
            "substance\n",
            "something\n",
            ".\n",
            "talk\n",
            "pollution\n",
            "earth\n",
            ",\n",
            "refer\n",
            "contamination\n",
            "happening\n",
            "natural\n",
            "resources\n",
            "various\n",
            "pollutants\n",
            ".\n",
            "mainly\n",
            "caused\n",
            "human\n",
            "activities\n",
            "harm\n",
            "environment\n",
            "ways\n",
            "one\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports the PorterStemmer class from the stem module of nltk.\n",
        "\n",
        "**Stemming** is the process of reducing words to their base or root form, often by removing suffixes.\n",
        "\n",
        "Stemming is commonly used in natural language processing tasks to reduce the dimensionality of the feature space by collapsing different variants of words into a single representation. However, it's important to note that stemming may produce imperfect results since it relies on heuristic rules to strip suffixes, and it may result in non-words or words with altered meanings.\n",
        "\n",
        "**stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]**  :\n",
        "This line uses a list comprehension to iterate over each word in the filtered_tokens list and applies the stemming operation using the stem method of the PorterStemmer object. The stemmed tokens are stored in the stemmed_tokens list."
      ],
      "metadata": {
        "id": "pDHMTNOd_Iiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "for token in stemmed_tokens :\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHrPDkLOwXqN",
        "outputId": "17ae6630-3a11-4ec4-88c0-df25bdddbf14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pollut\n",
            "term\n",
            "even\n",
            "kid\n",
            "awar\n",
            "day\n",
            ".\n",
            "becom\n",
            "common\n",
            "almost\n",
            "everyon\n",
            "acknowledg\n",
            "fact\n",
            "pollut\n",
            "rise\n",
            "continu\n",
            ".\n",
            "term\n",
            "'pollut\n",
            "'\n",
            "mean\n",
            "manifest\n",
            "unsolicit\n",
            "foreign\n",
            "substanc\n",
            "someth\n",
            ".\n",
            "talk\n",
            "pollut\n",
            "earth\n",
            ",\n",
            "refer\n",
            "contamin\n",
            "happen\n",
            "natur\n",
            "resourc\n",
            "variou\n",
            "pollut\n",
            ".\n",
            "mainli\n",
            "caus\n",
            "human\n",
            "activ\n",
            "harm\n",
            "environ\n",
            "way\n",
            "one\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports the WordNetLemmatizer class from the stem module of nltk.\n",
        "\n",
        "Lemmatization is the process of reducing words to their base or dictionary form(called lemma), which is linguistically valid.\n",
        "\n",
        "Lemmatization is preferred over stemming in many NLP applications because it produces valid words that are present in the language's dictionary. It helps maintain the semantic integrity of the text and is especially useful in tasks where word meanings and relationships need to be preserved, such as information retrieval, question answering, and sentiment analysis.\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens] :\n",
        "This line uses a list comprehension to iterate over each word in the filtered_tokens list and applies the lemmatization operation using the lemmatize method of the WordNetLemmatizer object. The lemmatized tokens are stored in the lemmatized_tokens list."
      ],
      "metadata": {
        "id": "ukME1dCaAuFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "for token in lemmatized_tokens :\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ea6vlK0wxmn",
        "outputId": "abb0a50f-6199-480b-a794-6234dd6bc882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pollution\n",
            "term\n",
            "even\n",
            "kid\n",
            "aware\n",
            "day\n",
            ".\n",
            "become\n",
            "common\n",
            "almost\n",
            "everyone\n",
            "acknowledges\n",
            "fact\n",
            "pollution\n",
            "rising\n",
            "continuously\n",
            ".\n",
            "term\n",
            "'pollution\n",
            "'\n",
            "mean\n",
            "manifestation\n",
            "unsolicited\n",
            "foreign\n",
            "substance\n",
            "something\n",
            ".\n",
            "talk\n",
            "pollution\n",
            "earth\n",
            ",\n",
            "refer\n",
            "contamination\n",
            "happening\n",
            "natural\n",
            "resource\n",
            "various\n",
            "pollutant\n",
            ".\n",
            "mainly\n",
            "caused\n",
            "human\n",
            "activity\n",
            "harm\n",
            "environment\n",
            "way\n",
            "one\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "imports the Tfidvectorizer class from the feature_extraction.text module of sklearn.\n",
        "\n",
        "TfidfVectorizer is used to transform a document into a TF-IDF (Term Frequency-Inverse Document Frequency) matrix.\n",
        "\n",
        "The TF-IDF matrix represents each document as a row and each term (word) as a column, with the TF-IDF score indicating the importance of the term in the document. TF-IDF scores are higher for terms that are more frequent in the document but less frequent across all documents in the corpus, reflecting their importance in distinguishing the document from others in the corpus.\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus) : This line applies the fit_transform method of the TfidfVectorizer object to the corpus ([document]). This method learns the vocabulary of the corpus and computes the TF-IDF scores for each term in each document. The result (tfidf_matrix) is a sparse matrix representation of the TF-IDF scores."
      ],
      "metadata": {
        "id": "NmXMjWaSDdHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [document]\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(tfidf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hul8Mi0xMDD",
        "outputId": "8e087112-804d-4012-c13a-b0f4de3482d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 36)\t0.08032193289024989\n",
            "  (0, 47)\t0.08032193289024989\n",
            "  (0, 32)\t0.08032193289024989\n",
            "  (0, 55)\t0.08032193289024989\n",
            "  (0, 16)\t0.08032193289024989\n",
            "  (0, 22)\t0.08032193289024989\n",
            "  (0, 2)\t0.08032193289024989\n",
            "  (0, 24)\t0.08032193289024989\n",
            "  (0, 10)\t0.08032193289024989\n",
            "  (0, 29)\t0.08032193289024989\n",
            "  (0, 51)\t0.08032193289024989\n",
            "  (0, 3)\t0.08032193289024989\n",
            "  (0, 37)\t0.08032193289024989\n",
            "  (0, 54)\t0.08032193289024989\n",
            "  (0, 9)\t0.16064386578049977\n",
            "  (0, 40)\t0.08032193289024989\n",
            "  (0, 33)\t0.08032193289024989\n",
            "  (0, 21)\t0.08032193289024989\n",
            "  (0, 12)\t0.08032193289024989\n",
            "  (0, 52)\t0.08032193289024989\n",
            "  (0, 39)\t0.08032193289024989\n",
            "  (0, 15)\t0.08032193289024989\n",
            "  (0, 35)\t0.08032193289024989\n",
            "  (0, 0)\t0.08032193289024989\n",
            "  (0, 45)\t0.08032193289024989\n",
            "  :\t:\n",
            "  (0, 31)\t0.08032193289024989\n",
            "  (0, 13)\t0.08032193289024989\n",
            "  (0, 41)\t0.08032193289024989\n",
            "  (0, 19)\t0.08032193289024989\n",
            "  (0, 49)\t0.4819315973414993\n",
            "  (0, 1)\t0.08032193289024989\n",
            "  (0, 18)\t0.08032193289024989\n",
            "  (0, 4)\t0.08032193289024989\n",
            "  (0, 48)\t0.24096579867074966\n",
            "  (0, 11)\t0.08032193289024989\n",
            "  (0, 42)\t0.08032193289024989\n",
            "  (0, 8)\t0.08032193289024989\n",
            "  (0, 23)\t0.08032193289024989\n",
            "  (0, 27)\t0.08032193289024989\n",
            "  (0, 14)\t0.08032193289024989\n",
            "  (0, 50)\t0.08032193289024989\n",
            "  (0, 34)\t0.24096579867074966\n",
            "  (0, 7)\t0.08032193289024989\n",
            "  (0, 6)\t0.08032193289024989\n",
            "  (0, 28)\t0.08032193289024989\n",
            "  (0, 17)\t0.08032193289024989\n",
            "  (0, 58)\t0.16064386578049977\n",
            "  (0, 46)\t0.16064386578049977\n",
            "  (0, 26)\t0.32128773156099955\n",
            "  (0, 38)\t0.32128773156099955\n"
          ]
        }
      ]
    }
  ]
}